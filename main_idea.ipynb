{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0eedb4a-e739-43b1-9c72-d9d5f9ad75b5",
   "metadata": {},
   "source": [
    "# Goal\n",
    "To generate polymers from the bottom up where the generation of monomers is implicitly guided to resemble a dataset of small molecules and where the polymer assembly is guided towards specified desired properties. Both the monomer and polymer level objectives are encouraged to increase diversity of the assembled chemical structure through a diversity object and also to obey the underlying chemical rules through a validity objective.\n",
    "\n",
    "This algorithm is designed as a reinforcement learning agent (RL agent) that operates within a chemistry aware graph generation environment. A molecule is successively constructed by either connecting a new substructure or an atom with an existing molecular graph or adding a bond to connect existing atoms. GCPN predicts the action of the bond addition, and is trained via policy gradient to optimize a reward composed of molecular property objectives and adversarial loss. The adversarial loss is provided by a graph convolutional network based discriminator trained jointly on a dataset of example molecules. Overall, this approach allows direct optimization of application-specific objectives, while ensuring that the generated molecules are realistic and satisfy chemical rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc4aa8-ca34-438c-bc36-17b18671a70d",
   "metadata": {},
   "source": [
    "# Problem Solution Details\n",
    "* Graph G is represented as: $G \\in (A, E, F)$\n",
    "* Adjacency Matrix A is represented as (assuming n nodes):  $A \\in\\{0,1\\}^{n \\times n}$\n",
    "* Node Feature Matrix F (assuming each node has d features): $F \\in \\mathbb{R}^{n \\times d}$\n",
    "* (Discrete) Edge-Conditioned Adjacency tensor (assuming b possible edge types): $E \\in\\{0,1\\}^{b \\times n \\times n}$\n",
    "    * $E_{i, j, k}=1$ if there exists an edge of type i between nodes j and k\n",
    "    * and $A=\\sum_{i=1}^b E_i$\n",
    "* Our primary objective is to generate graphs that maximize a given property function: $S(G) \\in \\mathbb{R}$ aka maximize \n",
    "    * $\\mathbb{E}_{G^{\\prime}}\\left[S\\left(G^{\\prime}\\right)\\right]$ where $G^{\\prime}$ is the generated graph and $S$ could be one or multiple domain-specific statistics of interest.\n",
    "* Constrain our model with two main sources of prior knowledge:\n",
    "    * (1) Generated graphs need to satisfy a set of hard constraints\n",
    "        * Chemical Valency\n",
    "    * (2) We provide the model with a set of example graphs $G \\sim p_{\\text {data }}(G)$ and would like to incorporate such prior knowledge by regularizing the property optimization objective with $\\mathbb{E}_{G, G^{\\prime}}\\left[J\\left(G, G^{\\prime}\\right)\\right]$ under distance metric $J(\\cdot, \\cdot)$\n",
    "        * this distance metric is an adversarially trained discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965482e4-b103-45c5-80a8-922100a88f6a",
   "metadata": {},
   "source": [
    "# Molecule Generation Environment\n",
    "The environment builds up a molecular graph step by step through a sequence of bond or substructure addition actions given by our agent.\n",
    "\n",
    "## State Space\n",
    "The state of the environment at timestep $t$ is $s_t$  and represents the intermediate generated graph $G_t$. This is fully observable by the agent. $G_0$ can be seen as the start of the generation process and starts as a single node that represents a carbon atom.\n",
    "\n",
    "## Action Space\n",
    "Special note: we must have a fixed-dimension and homogeneous action space to be amenable to the reinforcement learning framework.\n",
    "\n",
    "### Link Prediciton Action\n",
    "* set of scaffold subgraphs to be added during graph generation: $\\left\\{C_1, \\ldots, C_s\\right\\}$ which can be defined as $C=\\bigcup_{i=1}^s C_i$\n",
    "* Given a $G_t$ at step $t$ we define the corresponding extended graph as $G_t \\cup C$\n",
    "* Therefore an action can either correspond to connecting a new subgraph $C_i$ to a node $G_t$ or connecting existing nodes within the graph $G_t$\n",
    "* Once an action is taken, the remaining disconnected scaffold subgraphs are removed\n",
    "\n",
    "To start we will use the most fine-grained version of $C$ which consists of all $b$ different single node graphs, where $b$ denotes the number of different atom types (you can modify this without affecting the integrity of the algorithm). Also note that $C$ can be extended to contain molecule substructure scaffolds this will allow for specification of preferred substructures but at the cost of model flexibility.\n",
    "\n",
    "## Environment Dynamics\n",
    "Domain-specific rules are incorporated in the state transition dynamics. The environment carries out actions that obey the given rules. Infeasible actions proposed by the policy network are rejected and the state remains unchanged. For the task of molecule generation, the environment incorporates rules of chemistry.  All actions that attempt to update the environment according to the environment dynamics $p\\left(G_{t+1} \\mid G_t, a_t\\right)$ must (a) pass the valency check on the partial molecule (intermediate graph) according to the actions. Completed graphs must pass the valency check and also a (b) chemical validity check.\n",
    "\n",
    "Note the function below `check_valency` is used in the step-wise step-wise valency check conducted on the partial molecule (intermediate graph). Chemical validity checks on a final graph use the function `check_chemical_validity`.\n",
    "\n",
    "\n",
    "### Validation pieces of code\n",
    "```python\n",
    "from rdkit import Chem\n",
    "\n",
    "def check_chemical_validity(self, mol: Chem.rdchem.Mol):\n",
    "    \"\"\"\n",
    "    Checks the chemical validity of the mol object. Existing mol object is\n",
    "    not modified. Radicals pass this test.\n",
    "    :return: True if chemically valid, False otherwise\n",
    "    \"\"\"\n",
    "    s = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "    m = Chem.MolFromSmiles(s)  # implicitly performs sanitization\n",
    "    if m:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_valency(self, mol: Chem.rdchem.Mol):\n",
    "    \"\"\"\n",
    "    Checks that no atoms in the mol have exceeded their possible\n",
    "    valency\n",
    "    :return: True if no valency issues, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol,\n",
    "                         sanitizeOps=Chem.SanitizeFlags.SANITIZE_PROPERTIES)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "```\n",
    "\n",
    "## Reward Design\n",
    "Intermediate rewards and final rewards are used to guide the behaviour of the agent.\n",
    "* Final rewards as a sum over domain-specific rewards and adversarial rewards\n",
    "    * The domain-specific rewards consist of the (combination of) final property scores\n",
    "        * Domain-specific rewards also include penalization of unrealistic molecules according to various criteria, such as excessive steric strain and the presence of functional groups that violate ZINC functional group filters\n",
    "        * Note that the Domain-specific rewards are provided through a surrogate model trained according to [alghani 2022](https://arxiv.org/abs/2205.08619) using the [MPNN 2017](https://arxiv.org/abs/1704.01212) architecture implemented in [dgl-lifesci](https://github.com/awslabs/dgl-lifesci). For now this is fine as we want something non-experimental to train and get working; however, this will change when we consider 3D generation in November 2022. Perhaps we can even leverage pretrained 2D models in when considering 3D via methods like [St√§rk 3DInfoMax 2022](https://github.com/HannesStark/3DInfomax).\n",
    "        \n",
    "* Intermediate rewards include step-wise validity rewards and adversarial rewards\n",
    "    * A small positive reward is assigned if the action does not violate valency rules, otherwise a small negative reward is assigned\n",
    "\n",
    "When the environment updates according to a terminating action (selected by the agent), both a step reward and a final reward are given, and the generation process terminates.\n",
    "\n",
    "\n",
    "### Reward Scale\n",
    "When summing up all the rewards collected from a molecule generation trajectory, the range of the reward value that the model can get is $[-4,4]$, for final chemical property reward, $[-2,2]$ for final chemical filter reward, $[-1,1]$ for final adversarial reward, $[-1,1]$ for intermediate adversarial reward and $[-1,1]$ for intermediate validity reward.\n",
    "\n",
    "The philosophy behind the reward design, we linearly scale each reward component according to its importance in molecule generation from a chemistry point of view as well as the quality of generation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3075a82-9d39-4937-81af-212d869152dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Constrained Optimization\n",
    "## Adversarial Training\n",
    "TL;DR Goal of Adversarial Training is to incorporate prior knowledge specified by a dataset of example molecules.\n",
    "\n",
    "It is possible to hand code the rules or even to train a predictor for one of the properties; however, precisely representing the combination of these properties is extremely challenging. Adversarial training addresses the challenge through a learnable discriminator adversarially trained with a generator. After the training converges, the discriminator implicitly incorporates the information of a given dataset and guides the training of the generator.\n",
    "\n",
    "### Equation 1-A\n",
    "$$\n",
    "\\min _\\theta \\max _\\phi V\\left(\\pi_\\theta, D_\\phi\\right)=\\mathbb{E}_{x \\sim p_{\\text {data }}}\\left[\\log D_\\phi(x)\\right]+\\mathbb{E}_{x \\sim \\pi_\\theta}\\left[\\log D_\\phi(1-x)\\right]\n",
    "$$\n",
    "* $-V\\left(\\pi_\\theta, D_\\phi\\right)$ adversarial rewards from the GAN framework\n",
    "* $\\pi_\\theta$ is the policy network\n",
    "* $D_\\phi$ is the discriminator network\n",
    "* $x$ represents the input graph\n",
    "* $p_{\\text { data}}$ is the underlying data distribution which is defined either over final graphs (for final rewards) or intermediate graphs (for intermediate rewards)\n",
    "\n",
    "The discriminator network employs the same architecture as the policy network used to calculate node embeddings which are then aggregated into a graph level embedding and used to make a property prediction.\n",
    "\n",
    "Note this comes from the Minimax Loss function introduced in the [original GAN paper](https://arxiv.org/pdf/1406.2661.pdf): \n",
    "$\\min _G \\max _D V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data}}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]$\n",
    "\n",
    "The generator tries to minimize the following function while the discriminator tries to maximize it.\n",
    "* $D(x)$ is the discriminator's estimate of the probability that real data instance x is real.\n",
    "* $E_x$ is the expected value over all real data instances.\n",
    "* $G(z)$ is the generator's output when given noise z.\n",
    "* $D(G(z))$ is the discriminator's estimate of the probability that a fake instance is real.\n",
    "* $E_z$ is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances $G(z)$).\n",
    "\n",
    "The generator can't directly affect the $\\log D(\\boldsymbol{x})$ term in the function, so, for the generator, minimizing the loss is equivalent to minimizing $\\log (1-D(G(\\boldsymbol{z})))$.\n",
    "\n",
    "Note that if we stay with this style of constrained optimization we will likely move to a more modern GAN loss function like [EMD](https://en.wikipedia.org/wiki/Earth_mover's_distance)\n",
    "\n",
    "## MolDQN Similarity Score\n",
    "\n",
    "Similarity is defined as the Tanimoto similarity between Morgan fingerprints with radius 2 of the generated molecule $m$ and the original molecule $m_0$: $\\operatorname{SIM}\\left(m, m_0\\right) \\geq \\delta$ for a threshold $\\delta$\n",
    "\n",
    "[MolDQN](https://www.nature.com/articles/s41598-019-47148-x) trained a model in an environment whose initial state was randomly set to be one of the 800 molecules on the ZINC dataset which have the lowest penalized logP value, and ran the trained model on each molecule for one episode. The maximum number of steps per episode was limited to 20 in consideration of computational efficiency. In this task, the reward was designed as follows: \n",
    "\n",
    "$$\\mathcal{R}(s)= \\begin{cases}\\log \\mathrm{P}(m)-\\lambda \\times\\left(\\delta-\\operatorname{SIM}\\left(m, m_0\\right)\\right) & \\text { if } \\operatorname{SIM}\\left(m, m_0\\right)<\\delta \\\\ \\operatorname{logP}(m) & \\text { otherwise }\\end{cases}$$ where $\\lambda$ is the coefficient to balance the similarity and logP. If the similarity constraint is not satisfied, the reward is penalized by the difference between the target and current similarity. $\\lambda$ of 100 is used. \n",
    "\n",
    "### Equation 1-B\n",
    "\n",
    "Tanimoto similarity: $T(a, b)=\\frac{N_c}{N_a+N_b-N_c}$, $N_a$ and $N_b$ represents the number of attributes in each object (a, b) and $N_c$ is the number of attributes in common\n",
    "\n",
    "\n",
    "## Suspected Affect on Diversity of Discovered Chemical Structure\n",
    "We believe that constrained optimization has an affect on the diversity of discovered chemical structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7897e-97bf-4979-bd01-e1a7696d67a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graph Representation\n",
    "The graph generation environment uses a policy network learned by the RL agent to act in the environment and can take an intermediate graph $G_t$ and the collection of scaffold subgraphs $C$ as inputs, and outputs the action $a_t$, which predicts a new link to be added.\n",
    "\n",
    "## Computing Node Embeddings\n",
    "We first compute node embeddings of an input graph before we can perform link prediction $G_t \\cup C$. A GNN variant is used to support categorical edge types.\n",
    "\n",
    "The high-level idea is to perform message passing over each edge type for a total of $L$ layers. At the $l^{th}$ layer of the GNN all messages from different edge types are aggregated and used to compute the next layer node embedding $H^{(l+1)} \\in \\mathbb{R}^{(n+c) \\times k}$, where $n$ and $c$ are the sizes of $G_t$ and $C$ and $k$ (64 to start) is the embedding dimension.\n",
    "\n",
    "### Equation 2\n",
    "$$\n",
    "H^{(l+1)}=\\operatorname{AGG}\\left(\\operatorname{ReLU}\\left(\\left\\{\\tilde{D}_i^{-\\frac{1}{2}} \\tilde{E}_i \\tilde{D}_i^{-\\frac{1}{2}} H^{(l)} W_i^{(l)}\\right\\}, \\forall i \\in(1, \\ldots, b)\\right)\\right)\n",
    "$$\n",
    "\n",
    "* $E_i$ is the $i^{th}$ slice of the edge-conditioned adjacency tensor $E$\n",
    "* $\\tilde{E}_i=E_i+I$$\\tilde{D}_i=\\sum_k \\tilde{E}_{i j k}$ where $\\tilde{D}$ is a learnable degree matrix\n",
    "* $W_i^{(l)}$ is a trainable weight matrix for the $i^{th}$ edge type; using 4 to start (no bond, single bond, double bond, * bond)\n",
    "* $H^{(l)}$ is the node representation learned in the $l^{th}$ layer; we're using $L$ == 3 to start\n",
    "* $\\operatorname{AGG}(\\cdot)$ means some form of aggregation $\\{$ MEAN, MAX, SUM, CONCAT $\\}$...we'll try a few\n",
    "* The $L$ layer GNN is applied to the extended graph $G_t \\cup C$ to compute the final node embedding matrix $X=H^{(L)}$\n",
    "\n",
    "## Action Prediction\n",
    "\n",
    "The link prediction based action at $a_t$ time step $t$ is a concatenation of four components: selection of two nodes, prediction of edge type, and prediction of termination\n",
    "\n",
    "Concretely, each component is sampled according to a predicted distribution governed by equations 3 and 4.\n",
    "\n",
    "\n",
    "### Equation 3\n",
    "$$\n",
    "a_t=\\operatorname{CONCAT}\\left(a_{\\text {first }}, a_{\\text {second }}, a_{\\text {edge }}, a_{\\text {stop }}\\right)\n",
    "$$\n",
    "\n",
    "### Equation 4\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "f_{\\text {first }}\\left(s_t\\right) \\operatorname{SOFTMAX}\\left(m_f(X)\\right), & a_{\\text {first }} \\sim f_{\\text {first }}\\left(s_t\\right) \\in\\{0,1\\}^n \\\\\n",
    "f_{\\text {second }}\\left(s_t\\right)=\\operatorname{SOFTMAX}\\left(m_s\\left(X_{a_{\\text {first }}}, X\\right)\\right), & a_{\\text {second }} \\sim f_{\\text {second }}\\left(s_t\\right) \\in\\{0,1\\}^{n+c} \\\\\n",
    "f_{\\text {edge }}\\left(s_t\\right)=\\operatorname{SOFTMAX}\\left(m_e\\left(X_{a_{\\text {first }}}, X_{a_{\\text {second }}}\\right)\\right), & a_{\\text {edge }} \\sim f_{\\text {edge }}\\left(s_t\\right) \\in\\{0,1\\}^b \\\\\n",
    "f_{\\text {stop }}\\left(s_t\\right)=\\operatorname{SOFTMAX}\\left(m_t(\\mathrm{AGG}(X)),\\right. & a_{\\text {stop }} \\sim f_{\\text {stop }}\\left(s_t\\right) \\in\\{0,1\\}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### MLP 1\n",
    "$m_f$ to denote a MLP that maps $Z_{0: n} \\in \\mathbb{R}^{n \\times k}$ to $\\mathbb{R}^n$ vector which represents the probability of selecting each node. Information from the first selected\n",
    "node $a_{\\text {first }}$ is incorporated in the selection of the second node by concatenating its embedding $Z_{a_{\\text {first }}}$ with that of each node in $G_t \\cup C$\n",
    "\n",
    "#### MLP 2\n",
    "The second MLP $ms$ then maps the concatenated embedding to the probability distribution of each potential node to be selected as the second node. Note that when selecting two nodes to predict a link, the first node to select, $a_{\\text {first }}$, should always belong to the currently generated graph $G_t$, whereas the second node to select, $a_{\\text {second }}$, can be either from $G_t$ (forming a cycle), or from $C$ (adding a new substructure)\n",
    "\n",
    "#### MLP 3\n",
    "To predict a link $m_e$ takes $Z_{a_{\\text {first }}}$ and $Z_{a_{\\text {second }}}$ as inputs and maps to a categorical edge type using an MLP\n",
    "\n",
    "#### MLP 4\n",
    "The termination probability is computed by firstly aggregating the node embeddings into a graph embedding using an aggregation function $\\operatorname{AGG}$ then mapping the graph embedding to a scalar using an MLP $m_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3f6ca-4726-47ae-864c-d4e49e88476d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "* Reinforcement learning is capable of directly representing hard constraints and desired properties through the design of environment dynamics and reward function.\n",
    "* Reinforcement learning allows active exploration of the molecule space beyond samples in a dataset.\n",
    "\n",
    "## Policy Gradient Training\n",
    "Policy gradient based methods are widely adopted for optimizing policy networks. To start we're going to use Proximal Policy Optimization (PPO) for ease of implementation...lots of room for improvement here. Below equation 5 is the PPO objective function\n",
    "\n",
    "## Equation 5\n",
    "$$\n",
    "\\max L^{\\text {CLIP }}(\\theta)=\\mathbb{E}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right], r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{o l d}}\\left(a_t \\mid s_t\\right)}\n",
    "$$\n",
    "* $r_t(\\theta)$ is the probability ratio that is clipped to the range of $[1-\\epsilon, 1+\\epsilon]$\n",
    "* $L^{\\operatorname{CLIP}}(\\theta)$ is a lower bound of the conservative policy iteration objective\n",
    "* $\\hat{A}_t$ is the estimated advantage function which involves a learned value function $V_\\omega(\\cdot)$ to reduce the variance of estimation; note that $V_\\omega(\\cdot)$ is an MLP that maps the graph embedding computed according described in the section above\n",
    "\n",
    "### Imitation Learning\n",
    "\n",
    "Any ground truth molecule could be viewed as an expert trajectory for pretraining our agent. This expert imitation objective can be written as: $\\min L^{\\mathrm{EXPERT}}(\\theta)=-\\log \\left(\\pi_\\theta\\left(a_t \\mid s_t\\right)\\right)$ where $\\left(s_t, a_t\\right)$ pairs are obtained from ground truth molecules.\n",
    "\n",
    "Note: It is known that pretraining a policy network with expert policies if they are available leads to better training stability and performance...this is a very common first step in most rl training regimes.\n",
    "\n",
    "Given a molecule dataset, we randomly sample a molecular graph $G$, and randomly select one connected subgraph $G^{\\prime}$ of $G$ as the state $s_t$. At $s_t$ any action that adds an atom or bond in $G \\backslash G^{\\prime}$ can be taken in order to generate the sample molecule. Meaning randomly sample $a_t \\in G \\backslash G^{\\prime}$ and use $\\left(s_t, a_t\\right)$ pairs to supervise the expert imitation objective.\n",
    "\n",
    "#### Discuss the affect of expert imitation on molecular diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0a093-65a1-46e2-a1f0-09f660ed0d74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c79feb-798c-45d5-82aa-be266c9db7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
